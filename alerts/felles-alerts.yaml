apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tiltakspenger-felles-alerts
  namespace: {{namespace}}
  labels:
    team: {{team}}
spec:
  groups:
    - name: tiltakspenger-felles-alerts
      rules:
        - alert: Applikasjon er nede
          expr: sum by (deployment) (kube_deployment_status_replicas_available{namespace="{{namespace}}"}) == 0
          for: 5m
          annotations:
            consequence: '\{{ $labels.deployment }} har utilgjengelige podder'
            action: '`kubectl describe pod -l app=\{{ $labels.deployment }} -n \{{ $labels.namespace }}` for events og `kubectl get  pods -l app=\{{ $labels.deployment }} -n \{{ $labels.namespace }}` for å se feilende podder'
          labels:
            namespace: {{namespace}}
            severity: critical
        - alert: Høy feilrate i logger (> 5 feil siste timen og > 0 feil siste 15 min)
          expr: max(sum_over_time(loki:service:loglevel:count1m{detected_level="error", service_namespace="{{namespace}}", k8s_container_name=~"tiltakspenger-.*"}[1h])) by (service_name) > 5 and max(sum_over_time(loki:service:loglevel:count1m{detected_level="error", service_namespace="{{namespace}}", k8s_container_name=~"tiltakspenger-.*"}[15m])) by (service_name) > 0
          annotations:
            action: "Sjekk loggene til appen \{{ $labels.service_name }} for å se hvorfor det er så mye feil"
            dashboard_url: 'https://grafana.nav.cloud.nais.io/a/grafana-lokiexplore-app/explore/service/\{{ $labels.service_name }}/logs?var-filters=service_name%7C%3D%7C\{{ $labels.service_name }}&var-filters=service_namespace%7C%3D%7C{{namespace}}&from=now-3h&to=now&var-levels=detected_level%7C%3D%7Cerror&var-ds={{datasource}}'
          labels:
            namespace: {{namespace}}
            severity: critical
            datasource: {{datasource}}
        - alert: Kafka consumer offset lag
          expr: sum by(group, topic) (kafka_consumergroup_group_topic_sum_lag{group=~"^tiltakspenger-.*",group!="tiltakspenger-tiltak-consumer",client_id!="unknown"}) > 5
          for: 5m
          annotations:
            action: 'Konsument \{{ $labels.group }} har forsinkelse på lesing fra kafkatopic \{{ $labels.topic }}. Sjekk loggene for å finne ut hvorfor.'
          labels:
            namespace: {{namespace}}
            severity: warning
